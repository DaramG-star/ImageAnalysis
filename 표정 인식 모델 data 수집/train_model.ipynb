{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94786309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "laugh: 511 files\n",
      "none: 338 files\n",
      "serious: 509 files\n",
      "surprise: 539 files\n",
      "ugly: 403 files\n",
      "yawn: 292 files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '../face_data'\n",
    "subfolders = [f for f in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, f))]\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path = os.path.join(folder_path, subfolder)\n",
    "    file_count = len([f for f in os.listdir(subfolder_path) if os.path.isfile(os.path.join(subfolder_path, f))])\n",
    "    print(f\"{subfolder}: {file_count} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86405732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.3283 - loss: 1.5091 - val_accuracy: 0.6984 - val_loss: 0.9309\n",
      "Epoch 2/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.7260 - loss: 0.7759 - val_accuracy: 0.8137 - val_loss: 0.5115\n",
      "Epoch 3/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8523 - loss: 0.4561 - val_accuracy: 0.8803 - val_loss: 0.3714\n",
      "Epoch 4/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8974 - loss: 0.3017 - val_accuracy: 0.9091 - val_loss: 0.2785\n",
      "Epoch 5/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9514 - loss: 0.1791 - val_accuracy: 0.9113 - val_loss: 0.2509\n",
      "Epoch 6/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9520 - loss: 0.1528 - val_accuracy: 0.9424 - val_loss: 0.1959\n",
      "Epoch 7/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9767 - loss: 0.0897 - val_accuracy: 0.9446 - val_loss: 0.1932\n",
      "Epoch 8/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9761 - loss: 0.0913 - val_accuracy: 0.9446 - val_loss: 0.1996\n",
      "Epoch 9/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9729 - loss: 0.0669 - val_accuracy: 0.9424 - val_loss: 0.2176\n",
      "Epoch 10/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9877 - loss: 0.0436 - val_accuracy: 0.9534 - val_loss: 0.1818\n",
      "Epoch 11/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9910 - loss: 0.0384 - val_accuracy: 0.9468 - val_loss: 0.1995\n",
      "Epoch 12/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9904 - loss: 0.0356 - val_accuracy: 0.9557 - val_loss: 0.2114\n",
      "Epoch 13/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9874 - loss: 0.0379 - val_accuracy: 0.9601 - val_loss: 0.1647\n",
      "Epoch 14/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9931 - loss: 0.0265 - val_accuracy: 0.9601 - val_loss: 0.1650\n",
      "Epoch 15/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9963 - loss: 0.0146 - val_accuracy: 0.9623 - val_loss: 0.1763\n",
      "Epoch 16/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9950 - loss: 0.0151 - val_accuracy: 0.9490 - val_loss: 0.1881\n",
      "Epoch 17/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9870 - loss: 0.0465 - val_accuracy: 0.9623 - val_loss: 0.1753\n",
      "Epoch 18/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9946 - loss: 0.0190 - val_accuracy: 0.9690 - val_loss: 0.1348\n",
      "Epoch 19/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9946 - loss: 0.0227 - val_accuracy: 0.9667 - val_loss: 0.1580\n",
      "Epoch 20/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9944 - loss: 0.0193 - val_accuracy: 0.9623 - val_loss: 0.1481\n",
      "Epoch 21/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9965 - loss: 0.0108 - val_accuracy: 0.9645 - val_loss: 0.1614\n",
      "Epoch 22/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9976 - loss: 0.0091 - val_accuracy: 0.9667 - val_loss: 0.1637\n",
      "Epoch 23/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9993 - loss: 0.0076 - val_accuracy: 0.9601 - val_loss: 0.1886\n",
      "Epoch 24/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9974 - loss: 0.0120 - val_accuracy: 0.9690 - val_loss: 0.1357\n",
      "Epoch 25/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9976 - loss: 0.0066 - val_accuracy: 0.9690 - val_loss: 0.1537\n",
      "Epoch 26/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9980 - loss: 0.0085 - val_accuracy: 0.9667 - val_loss: 0.1604\n",
      "Epoch 27/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9942 - loss: 0.0145 - val_accuracy: 0.9645 - val_loss: 0.1696\n",
      "Epoch 28/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9977 - loss: 0.0081 - val_accuracy: 0.9690 - val_loss: 0.1667\n",
      "Epoch 29/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9972 - loss: 0.0098 - val_accuracy: 0.9667 - val_loss: 0.1539\n",
      "Epoch 30/30\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9977 - loss: 0.0091 - val_accuracy: 0.9645 - val_loss: 0.1602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# 1. 하이퍼파라미터\n",
    "IMG_SIZE = 64\n",
    "DATA_PATH = '../face_data'\n",
    "LABELS = ['laugh', 'serious', 'surprise', 'ugly', 'yawn']\n",
    "\n",
    "# 2. 데이터 불러오기\n",
    "X, y = [], []\n",
    "for idx, label in enumerate(LABELS):\n",
    "    folder_path = os.path.join(DATA_PATH, label)\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith('.jpg'):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)  # 흑백\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            X.append(img)\n",
    "            y.append(idx)\n",
    "\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1) / 255.0  # 정규화\n",
    "y = to_categorical(y)\n",
    "\n",
    "# 3. 학습/검증 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 4. 모델 구성\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(len(LABELS), activation='softmax')\n",
    "])\n",
    "\n",
    "# 5. 컴파일 & 학습\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# 6. 저장\n",
    "model.save('expression_cnn_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0a17086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpe8xtr57p\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpe8xtr57p\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\SSAFY\\AppData\\Local\\Temp\\tmpe8xtr57p'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 64, 64, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 5), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2256124633232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256124632352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256125405216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256125407504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256125454720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256125454192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256125459472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2256120727504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 저장된 모델 불러오기\n",
    "model = tf.keras.models.load_model('expression_cnn_model.h5')\n",
    "\n",
    "# 변환기 생성\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# 저장\n",
    "with open('expression_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f059c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 웹캠 실행 중... 얼굴 감지하여 표정 분류\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "\n",
    "LABELS = ['laugh', 'serious', 'surprise', 'ugly', 'yawn']\n",
    "IMG_SIZE = 64\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"expression_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Face detection 초기화\n",
    "mp_face = mp.solutions.face_detection\n",
    "face_detection = mp_face.FaceDetection(model_selection=0, min_detection_confidence=0.7)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"🎥 웹캠 실행 중... 얼굴 감지하여 표정 분류\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 얼굴 감지\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb)\n",
    "\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            h, w, _ = frame.shape\n",
    "            x, y, bw, bh = int(bboxC.xmin * w), int(bboxC.ymin * h), int(bboxC.width * w), int(bboxC.height * h)\n",
    "\n",
    "            # 얼굴 crop & resize\n",
    "            face = frame[y:y+bh, x:x+bw]\n",
    "            if face.size == 0:  # 잘라낸 이미지가 비어있는지 확인\n",
    "                continue        # 비어있으면 다음 프레임으로 넘어감\n",
    "            gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
    "            # 얼굴 이미지를 IMG_SIZE로 resize\n",
    "            resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "            input_data = resized.reshape(1, IMG_SIZE, IMG_SIZE, 1).astype(np.float32) / 255.0\n",
    "\n",
    "            # TFLite 추론\n",
    "            interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "            interpreter.invoke()\n",
    "            output = interpreter.get_tensor(output_details[0]['index'])\n",
    "            pred = np.argmax(output)\n",
    "            conf = np.max(output)\n",
    "\n",
    "            # 결과 출력\n",
    "            label = LABELS[pred]\n",
    "            cv2.putText(frame, f\"{label} ({conf:.2f})\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
    "    else:\n",
    "        cv2.putText(frame, \"No face detected\", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "\n",
    "    cv2.imshow(\"Expression Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47530833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎥 웹캠 실행 중... 'q'를 누르면 종료\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 64, 64, 1), dtype=float32). Expected shape (None, 936), but input has incompatible shape (1, 64, 64, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 64, 64, 1), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m input_data \u001b[38;5;241m=\u001b[39m resized\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, IMG_SIZE, IMG_SIZE, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# CNN 모델 추론\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m pred_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(pred_probs)\n\u001b[0;32m     29\u001b[0m conf \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(pred_probs)\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\SSAFY\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\models\\functional.py:276\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    274\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    275\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    277\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    279\u001b[0m     )\n\u001b[0;32m    280\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(1, 64, 64, 1), dtype=float32). Expected shape (None, 936), but input has incompatible shape (1, 64, 64, 1)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(1, 64, 64, 1), dtype=float32)\n  • training=False\n  • mask=None\n  • kwargs=<class 'inspect._empty'>"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 라벨\n",
    "LABELS = ['laugh', 'serious', 'surprise', 'ugly', 'yawn']\n",
    "IMG_SIZE = 64\n",
    "\n",
    "# ✅ Keras 모델 로드 (.h5)\n",
    "model = tf.keras.models.load_model('expression_cnn_model.h5')\n",
    "\n",
    "# 웹캠 실행\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"🎥 웹캠 실행 중... 'q'를 누르면 종료\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # 프레임 → 흑백 → resize → 정규화\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    resized = cv2.resize(gray, (IMG_SIZE, IMG_SIZE))\n",
    "    input_data = resized.reshape(1, IMG_SIZE, IMG_SIZE, 1).astype(np.float32) / 255.0\n",
    "\n",
    "    # CNN 모델 추론\n",
    "    pred_probs = model.predict(input_data, verbose=0)\n",
    "    pred = np.argmax(pred_probs)\n",
    "    conf = np.max(pred_probs)\n",
    "    label = LABELS[pred]\n",
    "\n",
    "    # 결과 화면에 표시\n",
    "    cv2.putText(frame, f\"{label} ({conf:.2f})\", (10, 40),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Expression Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
