import os
import re
import cv2
from icrawler.builtin import GoogleImageCrawler

# --------- ì„¤ì • ---------
keywords = ['ì—½ì‚¬']
SAVE_ROOT = 'crawled'
MAX_PER_KEYWORD = 100

face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
total_valid = 0

for keyword in keywords:
    # âœ… í•œê¸€ í‚¤ì›Œë“œëŠ” ê²€ìƒ‰ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©, í´ë”ëª…ì€ ì•ˆì „í•œ ì˜ë¬¸ìœ¼ë¡œ
    folder_name = 'ugly'  # â† í´ë”ëª…ì€ ì˜ë¬¸ìœ¼ë¡œ ê³ ì •!
    save_dir = os.path.join(SAVE_ROOT, folder_name)
    os.makedirs(save_dir, exist_ok=True)

    print(f"ğŸ” í¬ë¡¤ë§: '{keyword}' â†’ {save_dir}")
    crawler = GoogleImageCrawler(storage={'root_dir': save_dir})
    crawler.crawl(keyword=keyword, max_num=MAX_PER_KEYWORD)

    # âœ… ì–¼êµ´ í•„í„°ë§
    valid_count = 0
    for fname in os.listdir(save_dir):
        fpath = os.path.join(save_dir, fname)

        if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):
            os.remove(fpath)
            continue

        try:
            img = cv2.imread(fpath)
            if img is None or img.shape[0] < 50 or img.shape[1] < 50:
                os.remove(fpath)
                continue

            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=2)

            if len(faces) == 0:
                os.remove(fpath)
            else:
                valid_count += 1
        except Exception as e:
            print(f"âš ï¸ {fname} ì‚­ì œë¨: {e}")
            os.remove(fpath)

    print(f"âœ… ì–¼êµ´ í¬í•¨ëœ ì´ë¯¸ì§€: {valid_count}ì¥\n")
    total_valid += valid_count

print(f"ğŸ‰ ì „ì²´ ì–¼êµ´ í¬í•¨ëœ í•˜í’ˆ ì´ë¯¸ì§€ ìˆ˜: {total_valid}ì¥")
