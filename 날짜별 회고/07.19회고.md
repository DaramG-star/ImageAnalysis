# 📅 7/19 회고 – 손 제스처 분류 모델 개발

오늘은 손 제스처 분류 모델을 개선하는 데 집중했다.  
처음에는 Mediapipe의 Hand 모델을 사용하여 **관절의 절대 좌표**를 기반으로 모델을 만들었는데,  
**정확도가 낮고 손 위치에 따라 인식이 되지 않는 문제**가 있었다.

---

## 📸 시도 1: 이미지 기반 분류 (SVM)

- 각 제스처 당 500장씩, 총 약 4000장의 이미지를 수집
- SVM 등 전통 ML 모델로 학습
- **정확도: 약 70%**
- ❌ 손 위치/조명/배경에 따라 인식 실패율이 큼

---

## 🔄 시도 2: 관절 좌표 기반 모델

- Mediapipe 21개 관절의 x, y 좌표 추출
- **절대 좌표만 사용**한 경우, 이전과 유사한 문제가 발생함

---

## ✅ 해결책: 상대 좌표 + 특징 벡터화

- 관절 간의 상대 위치, 거리, 각도, 방향 벡터 등을 feature로 구성
- KNN 모델을 활용해 분류
- **정확도: 약 98% (테스트 기준)**  
- 실시간 반응성도 상당히 개선됨

---

## 🤔 인사이트

- **None(무 제스처) 데이터가 매우 중요하다는 걸 느낌**  
  → 가만히 있을 때나 손이 없는 상태에서 **오탐을 막기 위해 꼭 필요**
- 기존의 `rock`, `paper` 제스처는 실제 사용 시 꼭 필요하지 않을 수 있음  
  → 이들은 **None 카테고리로 합치는 방향 고려 중**

---

## 🔮 내일 할 일

- 다양한 각도, 조명, 손 위치에서 **추가 데이터 수집**
- `None` 클래스에 더 많은 데이터 보충
- `RNN 기반 연속 제스처 인식`도 테스트해보기  
  → 좀 더 **동적인 이모지 대응**을 위해 시도할 예정

---

## 🤬 깨달음

> 가만히 있는데 화면에 하트 이모지 뜨면 개빡친다  
> → **None 데이터는 아무리 많아도 부족하지 않다.**

