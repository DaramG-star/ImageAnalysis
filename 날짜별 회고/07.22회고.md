# 📅 7/22 회고 – LSTM으로 동적 손 제스처도 알아본다구! 🌀

오늘은 손 제스처가 움직이는 모습도 잘 인식할 수 있도록  
**시퀀스(연속된 동작)** 데이터를 기반으로 한 모델을 만들어봤어!  
이제는 손이 움직여도 "오 이건 하트구나~"라고 알아챌 수 있지! 💕

## 🎥 데이터 수집
먼저 영상 프레임에서 손 관절을 30프레임씩 모아서 하나의 제스처 시퀀스를 만들었어!  
총 **1540개의 시퀀스**를 수집했고, 각 프레임에서는 21개의 손 관절을 잡아냈지 ✋

- 각 관절의 x, y, z 좌표를 **손목 기준으로 상대좌표화**해서
- 21개 관절 × 3축 = 63차원 × 30프레임 = (30, 63) 형태로 데이터를 정리했어!

## 🧠 모델 구조 설명
움직이는 손 제스처를 다루는 거니까, 이번엔 **LSTM (순환 신경망)** 을 사용했어!

구성은 이렇게 되어 있어:
1. LSTM 레이어 2개로 시간적인 흐름을 기억하게 만들고
2. 중간에 Dropout으로 과적합 방지!
3. Dense 층을 지나서 마지막에 softmax로 제스처 종류를 예측했어

👉 총 분류할 제스처는 6가지였고, categorical crossentropy로 loss를 계산했지!

## 📈 학습 결과!
- **학습 정확도는 약 98%** 정도 나왔고, 검증 성능도 나쁘지 않았어!
- 다만... 아직 데이터 수가 조금 부족한 탓에 몇몇 제스처는 인식이 헷갈리는 경우가 있어 😥

## 💾 모델 저장과 변환
모델을 `.h5` 파일로 저장한 후, 바로 `tflite`로 변환해서 실시간 테스트까지 마쳤어!
이제 이 모델도 스마트폰이나 웹캠에서도 **빠르게 동작**할 수 있어! 🧃

## 🔧 향후 계획
- **데이터를 훨~씬 많이** 추가해서 정확도를 높일 예정이야 (최소 5천 시퀀스 목표!)
- 특히 **None 제스처**나 헷갈리는 동작들(hi, bye, fire 등)을 더 다양하게 넣을 거야
- 정적 모델과 동적 모델을 **함께 쓰는 구조**도 계속 고민 중이야  
  예: 손이 멈춰있을 땐 정적, 움직임이 감지되면 동적 모델이 동작하도록!

---

### 🤍 오늘의 깨달음
- 동적 제스처는 정말 **데이터가 핵심!**
- 비슷해보이는 동작이라도 속도나 방향이 다르면 인식이 어렵기 때문에, 다양한 상황을 고려해서 수집해야 돼!
- 두 모델을 잘 조합하면 이모지 시스템이 더 귀엽고 실용적으로 작동할 수 있을 것 같아 🐿️✨
